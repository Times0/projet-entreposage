# Report Data Protection – Project

## Introduction

The goal of this project is to use data from the physical and network datasets to predict the state of the system, it can either be 'normal' or under attack the types of attacks are:

- MITM (Man-in-the-middle)
- Physical fault
- DoS (Denial of Service)
- scan

The dataset is divided into 2 parts :
- Physical dataset : it contains 2 hours of 1 measurement per second data of sensors such as pressure, flow rate, and states of valves.
- Network dataset

We try to predict the state of the system using the physical dataset and the network dataset separately. We can 
already assume that the scanning attacks won't be correctly predicted using the physical dataset because they 
only affect the network data.

## Physical dataset
### Data preprocessing
We have 4 files for the physical dataset :
- phy_att_1,2 and 3 are tab separated csv files that contain each 30 minutes of data where attacks occurred.
- phy_att_4.csv is comma separated csv file that contains 30 minutes of data where attacks occurred.
- phy_normal.csv is tab separated csv file that contains 1 hour of normal data.

For our analysis we merged the 4 files into a single dataframe, this was not straightforward because some columns were not matching.

The files have slightly different column structures. While the first file only contains a "Label" column, files 2 and 3 include an additional binary indicator column "Label_n" (with some spelling variations like "Lable_n"). This indicator column uses a value of 1 to denote when the corresponding "Label" value represents an attack state, and 0 for normal operation.

proof :
```py
df1[(df1["Label_n"] == 1) & (df1["Label"] == "normal")]
>>> 0 rows × 43 columns
```

**What do we do ?** We remove the `label_n` and `lable_n` columns to merge the datasets correctly (42 columns) then we add a new column "Label_n" that will be 0 for normal and 1 for attack.

We also noticed an anormal attack name : "nomal". It is not an attack but a normal state, we replace it by "normal" for the whole dataframe.
![alt text](image.png)


We shuffle the dataframe to avoid any bias and drop the "Time" column because it is not relevant for our analysis. Indeed predicting the time of the attack is not the goal of this project.

### Model training

We have a choice to make : predicting the state of the system (normal or attack) or predicting the type of attack (MITM, DoS, Physical fault, Scan). 

![alt text](image-2.png)

We choose to first predict the state of the system then the type of attack, since the dataset seen like this is more balanced. Also predicting a scan attack or a MITM attack might be impossible only using the physical dataset.

We use the same training and testing conditions for each model according to the research paper : 
- 5-fold cross validation
- 80% of the data for training and 20% for testing

We run the same models that they used with the same hyperparameters.
<!-- ```py
models = {
    'KNN': KNeighborsClassifier(n_neighbors=10),
    'CART': DecisionTreeClassifier(random_state=15),
    'Random Forest': RandomForestClassifier(random_state=15, n_estimators=100),
    'XGBoost': XGBClassifier(random_state=15),
    'CatBoost': CatBoostClassifier(random_state=15, verbose=False),
    'MLP': MLPClassifier(random_state=15, max_iter=1000),
    "SVM": SVC(random_state=15, kernel="rbf"),
    "Naive Bayes": GaussianNB()
}
``` -->

- K-Nearest Neighbors (KNN) with k=10
- Random Forest with 100 trees
- Support Vector Machine (SVM) with RBF kernel
- Naive Bayes

Our training results and training time 

| Model | Accuracy | Recall | Precision | F1-Score |
|-------|----------|-----------|-----------|---------|
| KNN | 0.96 | 0.90 | 0.96 | 0.93 |
| Random Forest | 0.98 | 0.96 | 0.98 | 0.97 |
| SVM | 0.79 | 0.28 | 0.99 | 0.43 |
| Naive Bayes | 0.67 | 0.45 | 0.45 | 0.45 |

For comparison the paper had these results :
| Model | Accuracy | Recall | Precision | F1-Score |
|-------|----------|---------|-----------|----------|
| KNN | 0.98 | 0.95 | 0.95 | 0.95 |
| Random Forest | 0.99 | 0.98 | 0.95 | 0.97 |
| SVM | 0.93 | 0.92 | 0.64 | 0.75 |
| Naive Bayes | 0.93 | 0.92 | 0.66 | 0.77 |


We observe pretty similar results, the Random Forest and KNN have the best accuracy and F1-score while the SVM and Naive Bayes have the worst precision and recall.

We also some more modern models like XGBoost and CatBoost :
| Model | Accuracy | Recall | Precision | F1-Score |
|-------|----------|---------|-----------|----------|
| XGBoost | 0.98 | 0.97 | 0.98 | 0.97 |
| CatBoost | 0.98 | 0.96 | 0.97 | 0.97 |
